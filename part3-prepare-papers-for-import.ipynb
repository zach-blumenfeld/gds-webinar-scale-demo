{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d6c4f1",
   "metadata": {},
   "source": [
    "# Part 3: Formatting Papers for Neo4j Admin Import\n",
    "\n",
    "This notebook formats the papers nodes and cites relationships into csvs for admin import.  \n",
    "\n",
    "__Note: The runtime for this notebook depends greatly on the environment within which it is run.  It takes a few hours for me to complete on a 64-core 976 GB memory instance.__\n",
    "\n",
    "### Chunking Methodology\n",
    "This notebook splits the papers into chunks to avoid out of memory errors when formatting data.  The `chunk_size` variable determines the number of papers brought into memory at once. `chunk_size` can be adjusted as needed, specifically, it can be turned down if encountering kernel shutdowns due to running out of memory. \n",
    "\n",
    "### Reducing Dimensionality with PCA\n",
    "the PCA (128 component) object saved from Part 2 is used here to reduce the encoding vector from 768 to 128 dimensions.  This is a trade-off that gives up a small amount of variance in the original encodings for shorter vectors that will require less resource to work with in latter steps. It may be worth exploring higher dimensionality in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a66a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.lsc import MAG240MDataset\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from joblib import load\n",
    "\n",
    "ROOT_DATA_DIR = '/data'\n",
    "pca_model_file = f'{ROOT_DATA_DIR}/paper-feat-pca128.joblib'\n",
    "chunk_size = 20_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30235362",
   "metadata": {},
   "source": [
    "## Prepare Paper Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff9814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_pre_format_paper_data(dataset, from_ind, to_inx):\n",
    "    feat_in_memory = feat_in_memory = dataset.paper_feat[from_ind:to_inx]\n",
    "    feat_cols = [f'paper_encoding_{i}' for i in range(768)]\n",
    "    paper_df = pd.DataFrame(feat_in_memory, columns = feat_cols)\n",
    "    \n",
    "    paper_df['ogb_index'] = paper_df.index + from_ind\n",
    "    \n",
    "    paper_df['paper_subject'] = dataset.all_paper_label[from_ind:to_inx] \n",
    "    paper_df['paper_subject'] = paper_df['paper_subject'].fillna(-2)\n",
    "    \n",
    "    paper_df['paper_year'] = dataset.all_paper_year[from_ind:to_inx] \n",
    "    \n",
    "    split_dict = dataset.get_idx_split()\n",
    "    paper_df[\"split_segment\"] = 'REMAINDER'\n",
    "    paper_df.loc[paper_df.ogb_index.isin(split_dict['train']), 'split_segment'] = 'TRAIN'\n",
    "    paper_df.loc[paper_df.ogb_index.isin(split_dict['valid']), 'split_segment'] = 'VALIDATE'\n",
    "    paper_df.loc[paper_df.ogb_index.isin(split_dict['test-dev']), 'split_segment'] = 'TEST_DEV'\n",
    "    paper_df.loc[paper_df.ogb_index.isin(split_dict['test-challenge']), 'split_segment'] = 'TEST_CHALLENGE'\n",
    "    \n",
    "    paper_df['subject_status'] = \"ERROR\"\n",
    "    paper_df.loc[paper_df.paper_subject > -1,'subject_status'] = \"KNOWN\"\n",
    "    paper_df.loc[paper_df.paper_subject == -1,'subject_status'] = \"HIDDEN\"\n",
    "    paper_df.loc[paper_df.paper_subject == -2,'subject_status'] = \"UNKNOWN\"\n",
    "    \n",
    "    return paper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295562db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_paper_data(paper_df, pca_model_object = pca_model_file):\n",
    "    feat_cols = [f'paper_encoding_{i}' for i in range(768)]\n",
    "    feat_128_cols = ['paper_128_encoding_' + str(x) for x in range(128)]\n",
    "    pca128 = load(pca_model_object)\n",
    "    res_df = pd.DataFrame(pca128.transform(paper_df[feat_cols]), columns = feat_128_cols)\n",
    "    res_df = pd.concat([paper_df[[\"ogb_index\", \"split_segment\", \"subject_status\", \"paper_year\", \n",
    "                                           \"paper_subject\"]], res_df], axis=1)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8836134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Future Note: Change split_segment -> splitSegment and subject_status -> subjectStatus \n",
    "## for consitent naming and to work with next parts, namely export. \n",
    "def post_format_paper_data(reduced_paper_df, npartitions=2000):\n",
    "    feat_128_cols = ['paper_128_encoding_' + str(x) for x in range(128)]\n",
    "    reduced_paper_df.rename(\n",
    "        columns={\"ogb_index\":\"ogbIndex:ID\", \"split_segment\":\"split_segment:string\", \n",
    "                 \"subject_status\":\"subject_status:string\", \"paper_year\":\"year:int\", \n",
    "                 \"paper_subject\":\"subject:int\"}, inplace=True)\n",
    "    paper_ddf = dd.from_pandas(reduced_paper_df, npartitions=npartitions)\n",
    "    paper_ddf = paper_ddf.astype({'subject:int':'int32'})\n",
    "    paper_ddf[\"encoding:float[]\"] = \\\n",
    "    paper_ddf.apply(lambda x:\";\".join(['%0.5f' % i for i in x[feat_128_cols]]), axis=1,meta=(\"str\"))\n",
    "    paper_ddf = paper_ddf.drop(columns=feat_128_cols)\n",
    "    paper_ddf.compute(scheduler=\"processes\")\n",
    "    return paper_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf3760dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-formatting for chunk 0 to 20000000...\n",
      "pca reduction...\n",
      "post-formatting...\n",
      "writing chunk to files /data/demo-load/paper-c0-*.csv\n",
      "finished 16.43 of the data, iterated count to 1\n",
      "========================================\n",
      "========================================\n",
      "pre-formatting for chunk 20000000 to 40000000...\n",
      "pca reduction...\n",
      "post-formatting...\n",
      "writing chunk to files /data/demo-load/paper-c1-*.csv\n",
      "finished 32.85 of the data, iterated count to 2\n",
      "========================================\n",
      "========================================\n",
      "pre-formatting for chunk 40000000 to 60000000...\n",
      "pca reduction...\n",
      "post-formatting...\n",
      "writing chunk to files /data/demo-load/paper-c2-*.csv\n",
      "finished 49.28 of the data, iterated count to 3\n",
      "========================================\n",
      "========================================\n",
      "pre-formatting for chunk 60000000 to 80000000...\n",
      "pca reduction...\n",
      "post-formatting...\n",
      "writing chunk to files /data/demo-load/paper-c3-*.csv\n",
      "finished 65.71 of the data, iterated count to 4\n",
      "========================================\n",
      "========================================\n",
      "pre-formatting for chunk 80000000 to 100000000...\n",
      "pca reduction...\n",
      "post-formatting...\n",
      "writing chunk to files /data/demo-load/paper-c4-*.csv\n",
      "finished 82.13 of the data, iterated count to 5\n",
      "========================================\n",
      "========================================\n",
      "pre-formatting for chunk 100000000 to 120000000...\n",
      "pca reduction...\n",
      "post-formatting...\n",
      "writing chunk to files /data/demo-load/paper-c5-*.csv\n",
      "finished 98.56 of the data, iterated count to 6\n",
      "========================================\n",
      "========================================\n",
      "pre-formatting for chunk 120000000 to 140000000...\n",
      "pca reduction...\n",
      "post-formatting...\n",
      "writing chunk to files /data/demo-load/paper-c6-*.csv\n",
      "finished 114.99 of the data, iterated count to 7\n",
      "========================================\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "dataset = MAG240MDataset(root = ROOT_DATA_DIR)\n",
    "total_n = dataset.num_papers\n",
    "done_n = 0\n",
    "count = 0\n",
    "\n",
    "while done_n < total_n:\n",
    "    to_n = done_n + chunk_size\n",
    "    print(\"pre-formatting for chunk \" + str(done_n) + \" to \" + str(to_n) + \"...\")\n",
    "    paper_df = load_and_pre_format_paper_data(dataset, done_n, to_n)\n",
    "    print(\"pca reduction...\")\n",
    "    reduced_paper_df = reduce_paper_data(paper_df)\n",
    "    print(\"post-formatting...\")\n",
    "    paper_ddf = post_format_paper_data(reduced_paper_df)\n",
    "    print(\"writing chunk to files \" + f'{ROOT_DATA_DIR}/demo-load/paper-c{count}-*.csv')\n",
    "    paper_ddf = paper_ddf.repartition(npartitions=100)\n",
    "    paper_ddf.to_csv(f'{ROOT_DATA_DIR}/demo-load/paper-c{count}-*.csv', \n",
    "                          header_first_partition_only=True, index=False, compute_kwargs={'scheduler': 'processes'})\n",
    "    count += 1\n",
    "    done_n = to_n\n",
    "    print(f\"finished {round(100*done_n/total_n,2)} of the data, iterated count to {count}\")\n",
    "    print(\"========================================\")\n",
    "    print(\"========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9ea6a",
   "metadata": {},
   "source": [
    "## Prepare Cite Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5bd432",
   "metadata": {},
   "outputs": [],
   "source": [
    "cites_edge_ddf = dd.from_pandas(pd.DataFrame(dataset.edge_index('paper', 'paper').T, \n",
    "                                columns = [\":START_ID\",\":END_ID\"]), npartitions=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c159abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/graph2/lib/python3.9/site-packages/dask/dataframe/io/csv.py:916: FutureWarning: The 'scheduler' keyword argument for `to_csv()` is deprecated andwill be removed in a future version. Please use the `compute_kwargs` argument instead. For example, df.to_csv(..., compute_kwargs={scheduler: processes})\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/data/demo-load/cited-00.csv',\n",
       " '/data/demo-load/cited-01.csv',\n",
       " '/data/demo-load/cited-02.csv',\n",
       " '/data/demo-load/cited-03.csv',\n",
       " '/data/demo-load/cited-04.csv',\n",
       " '/data/demo-load/cited-05.csv',\n",
       " '/data/demo-load/cited-06.csv',\n",
       " '/data/demo-load/cited-07.csv',\n",
       " '/data/demo-load/cited-08.csv',\n",
       " '/data/demo-load/cited-09.csv',\n",
       " '/data/demo-load/cited-10.csv',\n",
       " '/data/demo-load/cited-11.csv',\n",
       " '/data/demo-load/cited-12.csv',\n",
       " '/data/demo-load/cited-13.csv',\n",
       " '/data/demo-load/cited-14.csv',\n",
       " '/data/demo-load/cited-15.csv',\n",
       " '/data/demo-load/cited-16.csv',\n",
       " '/data/demo-load/cited-17.csv',\n",
       " '/data/demo-load/cited-18.csv',\n",
       " '/data/demo-load/cited-19.csv',\n",
       " '/data/demo-load/cited-20.csv',\n",
       " '/data/demo-load/cited-21.csv',\n",
       " '/data/demo-load/cited-22.csv',\n",
       " '/data/demo-load/cited-23.csv',\n",
       " '/data/demo-load/cited-24.csv',\n",
       " '/data/demo-load/cited-25.csv',\n",
       " '/data/demo-load/cited-26.csv',\n",
       " '/data/demo-load/cited-27.csv',\n",
       " '/data/demo-load/cited-28.csv',\n",
       " '/data/demo-load/cited-29.csv',\n",
       " '/data/demo-load/cited-30.csv',\n",
       " '/data/demo-load/cited-31.csv',\n",
       " '/data/demo-load/cited-32.csv',\n",
       " '/data/demo-load/cited-33.csv',\n",
       " '/data/demo-load/cited-34.csv',\n",
       " '/data/demo-load/cited-35.csv',\n",
       " '/data/demo-load/cited-36.csv',\n",
       " '/data/demo-load/cited-37.csv',\n",
       " '/data/demo-load/cited-38.csv',\n",
       " '/data/demo-load/cited-39.csv',\n",
       " '/data/demo-load/cited-40.csv',\n",
       " '/data/demo-load/cited-41.csv',\n",
       " '/data/demo-load/cited-42.csv',\n",
       " '/data/demo-load/cited-43.csv',\n",
       " '/data/demo-load/cited-44.csv',\n",
       " '/data/demo-load/cited-45.csv',\n",
       " '/data/demo-load/cited-46.csv',\n",
       " '/data/demo-load/cited-47.csv',\n",
       " '/data/demo-load/cited-48.csv',\n",
       " '/data/demo-load/cited-49.csv',\n",
       " '/data/demo-load/cited-50.csv',\n",
       " '/data/demo-load/cited-51.csv',\n",
       " '/data/demo-load/cited-52.csv',\n",
       " '/data/demo-load/cited-53.csv',\n",
       " '/data/demo-load/cited-54.csv',\n",
       " '/data/demo-load/cited-55.csv',\n",
       " '/data/demo-load/cited-56.csv',\n",
       " '/data/demo-load/cited-57.csv',\n",
       " '/data/demo-load/cited-58.csv',\n",
       " '/data/demo-load/cited-59.csv',\n",
       " '/data/demo-load/cited-60.csv',\n",
       " '/data/demo-load/cited-61.csv',\n",
       " '/data/demo-load/cited-62.csv',\n",
       " '/data/demo-load/cited-63.csv',\n",
       " '/data/demo-load/cited-64.csv',\n",
       " '/data/demo-load/cited-65.csv',\n",
       " '/data/demo-load/cited-66.csv',\n",
       " '/data/demo-load/cited-67.csv',\n",
       " '/data/demo-load/cited-68.csv',\n",
       " '/data/demo-load/cited-69.csv',\n",
       " '/data/demo-load/cited-70.csv',\n",
       " '/data/demo-load/cited-71.csv',\n",
       " '/data/demo-load/cited-72.csv',\n",
       " '/data/demo-load/cited-73.csv',\n",
       " '/data/demo-load/cited-74.csv',\n",
       " '/data/demo-load/cited-75.csv',\n",
       " '/data/demo-load/cited-76.csv',\n",
       " '/data/demo-load/cited-77.csv',\n",
       " '/data/demo-load/cited-78.csv',\n",
       " '/data/demo-load/cited-79.csv',\n",
       " '/data/demo-load/cited-80.csv',\n",
       " '/data/demo-load/cited-81.csv',\n",
       " '/data/demo-load/cited-82.csv',\n",
       " '/data/demo-load/cited-83.csv',\n",
       " '/data/demo-load/cited-84.csv',\n",
       " '/data/demo-load/cited-85.csv',\n",
       " '/data/demo-load/cited-86.csv',\n",
       " '/data/demo-load/cited-87.csv',\n",
       " '/data/demo-load/cited-88.csv',\n",
       " '/data/demo-load/cited-89.csv',\n",
       " '/data/demo-load/cited-90.csv',\n",
       " '/data/demo-load/cited-91.csv',\n",
       " '/data/demo-load/cited-92.csv',\n",
       " '/data/demo-load/cited-93.csv',\n",
       " '/data/demo-load/cited-94.csv',\n",
       " '/data/demo-load/cited-95.csv',\n",
       " '/data/demo-load/cited-96.csv',\n",
       " '/data/demo-load/cited-97.csv',\n",
       " '/data/demo-load/cited-98.csv',\n",
       " '/data/demo-load/cited-99.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cites_edge_ddf.to_csv(f'{ROOT_DATA_DIR}/demo-load/cited-*.csv', \n",
    "                      header_first_partition_only=True, index=False, scheduler=\"processes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d46072",
   "metadata": {},
   "source": [
    "### Removing Extra Headers for Papers\n",
    "\n",
    "Unfortunately a small manual step here since I didn't get the headings quite right when writing the paper csvs. There should only be one paper csv with a header (the first one from the first chunk) for admin import. However, In the chunking logic above I write a header for the first file of each chunk. As a result, we must remove the headers from the first file of each chunk with exception to the initial chunk.  \n",
    "\n",
    "It is easy to accomplish this in a terminal.  Simply go to the directory with the csvs and execute the `sed` command like below for each first file with exception to `paper-c0-00.csv`.  For the chunk size of 20 Million, it would look like the below. \n",
    "\n",
    "```bash\n",
    "  sed -i '1d' paper-c1-00.csv \n",
    "  sed -i '1d' paper-c2-00.csv \n",
    "  sed -i '1d' paper-c3-00.csv \n",
    "  sed -i '1d' paper-c4-00.csv \n",
    "  sed -i '1d' paper-c5-00.csv \n",
    "  sed -i '1d' paper-c6-00.csv \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3f9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
